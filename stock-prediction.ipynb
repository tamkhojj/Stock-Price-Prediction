{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11361787,"sourceType":"datasetVersion","datasetId":7111198},{"sourceId":11494269,"sourceType":"datasetVersion","datasetId":7205437}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pandas numpy scikit-learn xgboost matplotlib seaborn joblib ta tqdm keras pmdarima statsmodels tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:17:27.842654Z","iopub.execute_input":"2025-04-21T13:17:27.842970Z","iopub.status.idle":"2025-04-21T13:17:31.300422Z","shell.execute_reply.started":"2025-04-21T13:17:27.842946Z","shell.execute_reply":"2025-04-21T13:17:31.299347Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.5)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.12.2)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.4.2)\nRequirement already satisfied: ta in /usr/local/lib/python3.10/dist-packages (0.11.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\nRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.5.0)\nRequirement already satisfied: pmdarima in /usr/local/lib/python3.10/dist-packages (2.0.4)\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (0.14.4)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\nRequirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.12.1)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.13.1)\nRequirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.1)\nRequirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.11)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.3.0)\nRequirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (75.1.0)\nRequirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels) (1.0.1)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam, RMSprop, SGD, Adamax\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom joblib import dump, load\nfrom datetime import timedelta\n\n# Define model names and create directories\nMODEL_NAMES = ['RandomForest', 'XGBoost', 'LinearRegression', 'ARIMA', 'LSTM']\nfor model in MODEL_NAMES:\n    os.makedirs(f\"trained_models/{model}\", exist_ok=True)\n    os.makedirs(f\"charts/{model}\", exist_ok=True)\nos.makedirs(\"charts/comparison\", exist_ok=True)\nos.makedirs(\"charts/LSTM/optimizers\", exist_ok=True)\nos.makedirs(\"charts/forecast\", exist_ok=True)\n\ndef load_json_file(filepath):\n    with open(filepath, 'r') as f:\n        return pd.DataFrame(json.load(f))\n\ndef evaluate_model(y_true, y_pred, prefix=\"\"):\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"{prefix} MAE: {mae:.2f}\")\n    print(f\"{prefix} MSE: {mse:.2f}\")\n    print(f\"{prefix} RMSE: {rmse:.2f}\")\n    print(f\"{prefix} R²: {r2:.2f}\")\n    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n\ndef plot_predictions(df, actual, train_pred, test_pred, title, output_path, seq_len=50):\n    plt.figure(figsize=(15, 8))\n    plt.plot(df.index[seq_len:], actual[seq_len:], label=\"Actual Prices\", color='green')\n    plt.plot(df.index[seq_len:seq_len+len(train_pred)], train_pred, label=\"Train Predictions\", color='orange')\n    plt.plot(df.index[seq_len+len(train_pred):seq_len+len(train_pred)+len(test_pred)], test_pred, label=\"Test Predictions\", color='teal')\n    plt.legend()\n    plt.title(title)\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Price\")\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_training_history(history, model_name, stock_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'{stock_name} - {model_name} Training History')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.savefig(f\"charts/{model_name}/{stock_name}_training_history.png\")\n    plt.close()\n\ndef plot_optimizer_comparison(optimizer_scores, stock_name):\n    plt.figure(figsize=(12, 6))\n    metrics = ['MAE', 'MSE', 'RMSE', 'R2']\n    for metric in metrics:\n        values = [scores[metric] for opt, scores in optimizer_scores.items()]\n        plt.plot(values, label=metric)\n    plt.xticks(range(len(optimizer_scores)), optimizer_scores.keys())\n    plt.title(f'{stock_name} - LSTM Optimizer Comparison')\n    plt.xlabel('Optimizer')\n    plt.ylabel('Metric Value')\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(f\"charts/LSTM/optimizers/{stock_name}_optimizer_comparison.png\")\n    plt.close()\n\ndef plot_model_comparison(all_scores, metric, output_path, stock_name=\"HPG\"):\n    plt.figure(figsize=(8, 6))\n    stock_names = list(all_scores.keys())\n    \n    if not stock_names:\n        print(f\"No data to plot for {metric}\")\n        plt.close()\n        return\n\n    scores = all_scores[stock_names[0]]\n    model_names = []\n    values = []\n\n    for model_name in MODEL_NAMES:\n        score = scores.get(model_name)\n        if isinstance(score, dict) and metric in score:\n            model_names.append(model_name)\n            values.append(score[metric])\n\n    if not values:\n        print(f\"No valid data to plot for {metric}\")\n        plt.close()\n        return\n\n    plt.bar(model_names, values, color=['blue', 'orange', 'red', 'green', 'purple'])\n    plt.title(f'Model Comparison - {metric} ({stock_name})')\n    plt.xlabel('Model')\n    plt.ylabel(metric)\n    plt.grid(axis='y')\n    plt.xticks(rotation=45)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_forecast_1_day(df, actual, forecast_1, model_name, stock_name, last_date):\n    plt.figure(figsize=(15, 8))\n    plt.plot(df.index, actual, label=\"Historical Prices\", color='green')\n    future_dates_1 = [last_date + timedelta(days=1)]\n    plt.plot(future_dates_1, [forecast_1], 'ro', label=\"1-Day Forecast\")\n    plt.legend()\n    plt.title(f\"{stock_name} - {model_name} Price Forecast (1 Day)\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Price\")\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(f\"charts/forecast/{stock_name}_{model_name}_forecast_1_day.png\")\n    plt.close()\n\ndef plot_forecast_15_days(df, actual, forecast_15, model_name, stock_name, last_date):\n    plt.figure(figsize=(15, 8))\n    plt.plot(df.index, actual, label=\"Historical Prices\", color='green')\n    future_dates_15 = [last_date + timedelta(days=i+1) for i in range(15)]\n    plt.plot(future_dates_15, forecast_15, 'b-', label=\"15-Day Forecast\")\n    plt.legend()\n    plt.title(f\"{stock_name} - {model_name} Price Forecast (15 Days)\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Price\")\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(f\"charts/forecast/{stock_name}_{model_name}_forecast_15_days.png\")\n    plt.close()\n\ndef plot_forecast_30_days(df, actual, forecast_30, model_name, stock_name, last_date):\n    plt.figure(figsize=(15, 8))\n    plt.plot(df.index, actual, label=\"Historical Prices\", color='green')\n    future_dates_30 = [last_date + timedelta(days=i+1) for i in range(30)]\n    plt.plot(future_dates_30, forecast_30, 'purple', label=\"30-Day Forecast\")\n    plt.legend()\n    plt.title(f\"{stock_name} - {model_name} Price Forecast (30 Days)\")\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Price\")\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(f\"charts/forecast/{stock_name}_{model_name}_forecast_30_days.png\")\n    plt.close()\n\ndef create_sequences(data, seq_len, forecast_steps=1, target_col_idx=3):  # Default target is 'Close'\n    X, y = [], []\n    for i in range(len(data) - seq_len - forecast_steps + 1):\n        X.append(data[i:i+seq_len])  # All features for seq_len time steps\n        y.append(data[i+seq_len:i+seq_len+forecast_steps, target_col_idx])  # Target is 'Close'\n    return np.array(X), np.array(y)\n\ndef calculate_trend_slope(close_prices, window=30):\n    if len(close_prices) < window:\n        return 0\n    recent_prices = close_prices[-window:]\n    x = np.arange(len(recent_prices))\n    slope, _ = np.polyfit(x, recent_prices, 1)\n    slope = np.clip(slope, -0.005, 0.005)\n    return slope\n\ndef estimate_arima_order(data, max_p=5, max_d=2, max_q=5):\n    d = 0\n    temp_data = data.copy()\n    while d <= max_d:\n        if d > 0:\n            temp_data = np.diff(temp_data)\n        if len(temp_data) > 1:\n            variance = np.var(temp_data)\n            if variance < 0.01 or d == max_d:\n                break\n        d += 1\n    p, q = 1, 1  # Simplified; consider using auto_arima for better order selection\n    return p, d, q\n\ndef train_and_forecast(filepath, seq_len=50, forecast_horizons=[1, 15, 30]):\n    # Load and preprocess data\n    df = load_json_file(filepath)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    df = df.dropna()\n\n    if df.shape[0] < seq_len + max(forecast_horizons):\n        print(f\"Skipping {filepath}: Insufficient data (<{seq_len + max(forecast_horizons)} rows).\")\n        return None\n\n    # Define feature columns\n    feature_columns = ['Open', 'High', 'Low', 'Close', 'Volume', \n                       'SMA_5', 'SMA_20', 'EMA_5', 'EMA_20', \n                       'RSI_14', 'MACD', 'Signal_Line', \n                       'Upper_Band', 'Lower_Band', 'ATR_14', 'OBV']\n    \n    # Apply log transformation to price-related features\n    price_cols = ['Open', 'High', 'Low', 'Close', 'Volume', \n                  'SMA_5', 'SMA_20', 'EMA_5', 'EMA_20', \n                  'Upper_Band', 'Lower_Band']\n    for col in price_cols:\n        df[col] = np.log1p(df[col].clip(lower=0))  # Ensure non-negative values\n\n    # Scale all features\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df[feature_columns])\n    dump(scaler, f\"trained_models/scaler_{os.path.basename(filepath).replace('.json', '')}.joblib\")\n\n    # Create sequences\n    X_scaled, y_scaled = create_sequences(scaled_data, seq_len, forecast_steps=1, target_col_idx=3)  # Close is at index 3\n    print(\"X shape:\",X_scaled.shape)\n    print(\"Y shape\", y_scaled.shape)\n    # Split into train and test sets\n    train_size = int(len(X_scaled) * 0.8)\n    X_train_scaled, X_test_scaled = X_scaled[:train_size], X_scaled[train_size:]\n    y_train_scaled, y_test_scaled = y_scaled[:train_size], y_scaled[train_size:]\n\n    model_scores = {}\n    stock_name = os.path.basename(filepath).replace('.json', '')\n    last_date = df.index[-1]\n    last_sequence_scaled = scaled_data[-seq_len:]  # All features for the last sequence\n\n    # Calculate trend slope and volatility based on Close prices\n    close_prices = np.log1p(df['Close'].values)\n    trend_slope = calculate_trend_slope(close_prices, window=30)\n    volatility = np.std(close_prices[-30:]) / np.mean(close_prices[-30:]) if len(close_prices) >= 30 else 0.01\n\n    # Random Forest\n    rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_split=10, min_samples_leaf=4, random_state=42)\n    rf.fit(X_train_scaled.reshape(X_train_scaled.shape[0], -1), y_train_scaled.flatten())\n    dump(rf, f\"trained_models/RandomForest/{stock_name}.joblib\")\n    y_rf_train_scaled = rf.predict(X_train_scaled.reshape(X_train_scaled.shape[0], -1))\n    y_rf_test_scaled = rf.predict(X_test_scaled.reshape(X_test_scaled.shape[0], -1))\n    \n    # Inverse transform predictions\n    y_rf_train = scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_rf_train_scaled), len(feature_columns)-1)), \n                        y_rf_train_scaled.reshape(-1, 1)], axis=1))[:, 3]\n    y_rf_test = scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_rf_test_scaled), len(feature_columns)-1)), \n                        y_rf_test_scaled.reshape(-1, 1)], axis=1))[:, 3]\n    y_rf_train = np.expm1(y_rf_train)\n    y_rf_test = np.expm1(y_rf_test)\n    y_test_original = np.expm1(scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_test_scaled), len(feature_columns)-1)), \n                        y_test_scaled.reshape(-1, 1)], axis=1))[:, 3])\n    \n    model_scores[\"RandomForest\"] = evaluate_model(y_test_original, y_rf_test, \"Random Forest Test\")\n    plot_predictions(df, df['Close'], y_rf_train, y_rf_test, \n                     f\"{stock_name} - Random Forest\", f\"charts/RandomForest/{stock_name}.png\", seq_len)\n\n    # Random Forest Forecast\n    forecasts_rf = {}\n    current_sequence = last_sequence_scaled.copy()\n    for horizon in forecast_horizons:\n        predictions = []\n        temp_sequence = current_sequence.copy()\n        for i in range(horizon):\n            pred_scaled = rf.predict(temp_sequence.reshape(1, -1))\n            pred = scaler.inverse_transform(\n                np.concatenate([np.zeros((1, len(feature_columns)-1)), \n                                pred_scaled.reshape(-1, 1)], axis=1))[:, 3]\n            pred = np.expm1(pred)[0]\n            pred = max(0, pred + (1 - np.exp(-0.1 * i)) * trend_slope + np.random.normal(0, volatility * pred))\n            predictions.append(pred)\n            # Update sequence with predicted Close price (simplified)\n            scaled_pred = scaler.transform(\n                np.concatenate([temp_sequence[-1, :-1].reshape(1, -1), \n                                np.log1p([[pred]]).reshape(1, -1)], axis=1))\n            temp_sequence = np.roll(temp_sequence, -1, axis=0)\n            temp_sequence[-1] = scaled_pred\n        forecasts_rf[f\"{horizon}_days\"] = predictions\n    plot_forecast_1_day(df, df['Close'], forecasts_rf[\"1_days\"][0], \"RandomForest\", stock_name, last_date)\n    plot_forecast_15_days(df, df['Close'], forecasts_rf[\"15_days\"], \"RandomForest\", stock_name, last_date)\n    plot_forecast_30_days(df, df['Close'], forecasts_rf[\"30_days\"], \"RandomForest\", stock_name, last_date)\n\n    # XGBoost\n    xgb = XGBRegressor(n_estimators=200, learning_rate=0.05, max_depth=7, min_child_weight=3, reg_lambda=1.5, random_state=42)\n    xgb.fit(X_train_scaled.reshape(X_train_scaled.shape[0], -1), y_train_scaled.flatten())\n    dump(xgb, f\"trained_models/XGBoost/{stock_name}.joblib\")\n    y_xgb_train_scaled = xgb.predict(X_train_scaled.reshape(X_train_scaled.shape[0], -1))\n    y_xgb_test_scaled = xgb.predict(X_test_scaled.reshape(X_test_scaled.shape[0], -1))\n    \n    y_xgb_train = scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_xgb_train_scaled), len(feature_columns)-1)), \n                        y_xgb_train_scaled.reshape(-1, 1)], axis=1))[:, 3]\n    y_xgb_test = scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_xgb_test_scaled), len(feature_columns)-1)), \n                        y_xgb_test_scaled.reshape(-1, 1)], axis=1))[:, 3]\n    y_xgb_train = np.expm1(y_xgb_train)\n    y_xgb_test = np.expm1(y_xgb_test)\n    \n    model_scores[\"XGBoost\"] = evaluate_model(y_test_original, y_xgb_test, \"XGBoost Test\")\n    plot_predictions(df, df['Close'], y_xgb_train, y_xgb_test, \n                     f\"{stock_name} - XGBoost\", f\"charts/XGBoost/{stock_name}.png\", seq_len)\n\n    # XGBoost Forecast\n    forecasts_xgb = {}\n    current_sequence = last_sequence_scaled.copy()\n    for horizon in forecast_horizons:\n        predictions = []\n        temp_sequence = current_sequence.copy()\n        for i in range(horizon):\n            pred_scaled = xgb.predict(temp_sequence.reshape(1, -1))\n            pred = scaler.inverse_transform(\n                np.concatenate([np.zeros((1, len(feature_columns)-1)), \n                                pred_scaled.reshape(-1, 1)], axis=1))[:, 3]\n            pred = np.expm1(pred)[0]\n            pred = max(0, pred + (1 - np.exp(-0.1 * i)) * trend_slope + np.random.normal(0, volatility * pred))\n            predictions.append(pred)\n            scaled_pred = scaler.transform(\n                np.concatenate([temp_sequence[-1, :-1].reshape(1, -1), \n                                np.log1p([[pred]]).reshape(1, -1)], axis=1))\n            temp_sequence = np.roll(temp_sequence, -1, axis=0)\n            temp_sequence[-1] = scaled_pred\n        forecasts_xgb[f\"{horizon}_days\"] = predictions\n    plot_forecast_1_day(df, df['Close'], forecasts_xgb[\"1_days\"][0], \"XGBoost\", stock_name, last_date)\n    plot_forecast_15_days(df, df['Close'], forecasts_xgb[\"15_days\"], \"XGBoost\", stock_name, last_date)\n    plot_forecast_30_days(df, df['Close'], forecasts_xgb[\"30_days\"], \"XGBoost\", stock_name, last_date)\n\n    # Linear Regression\n    lr = LinearRegression()\n    lr.fit(X_train_scaled.reshape(X_train_scaled.shape[0], -1), y_train_scaled.flatten())\n    dump(lr, f\"trained_models/LinearRegression/{stock_name}.joblib\")\n    y_lr_train_scaled = lr.predict(X_train_scaled.reshape(X_train_scaled.shape[0], -1))\n    y_lr_test_scaled = lr.predict(X_test_scaled.reshape(X_test_scaled.shape[0], -1))\n    \n    y_lr_train = scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_lr_train_scaled), len(feature_columns)-1)), \n                        y_lr_train_scaled.reshape(-1, 1)], axis=1))[:, 3]\n    y_lr_test = scaler.inverse_transform(\n        np.concatenate([np.zeros((len(y_lr_test_scaled), len(feature_columns)-1)), \n                        y_lr_test_scaled.reshape(-1, 1)], axis=1))[:, 3]\n    y_lr_train = np.expm1(y_lr_train)\n    y_lr_test = np.expm1(y_lr_test)\n    \n    model_scores[\"LinearRegression\"] = evaluate_model(y_test_original, y_lr_test, \"Linear Regression Test\")\n    plot_predictions(df, df['Close'], y_lr_train, y_lr_test, \n                     f\"{stock_name} - Linear Regression\", f\"charts/LinearRegression/{stock_name}.png\", seq_len)\n\n    # Linear Regression Forecast\n    forecasts_lr = {}\n    current_sequence = last_sequence_scaled.copy()\n    for horizon in forecast_horizons:\n        predictions = []\n        temp_sequence = current_sequence.copy()\n        for i in range(horizon):\n            pred_scaled = lr.predict(temp_sequence.reshape(1, -1))\n            pred = scaler.inverse_transform(\n                np.concatenate([np.zeros((1, len(feature_columns)-1)), \n                                pred_scaled.reshape(-1, 1)], axis=1))[:, 3]\n            pred = np.expm1(pred)[0]\n            pred = max(0, pred + (1 - np.exp(-0.1 * i)) * trend_slope + np.random.normal(0, volatility * pred))\n            predictions.append(pred)\n            scaled_pred = scaler.transform(\n                np.concatenate([temp_sequence[-1, :-1].reshape(1, -1), \n                                np.log1p([[pred]]).reshape(1, -1)], axis=1))\n            temp_sequence = np.roll(temp_sequence, -1, axis=0)\n            temp_sequence[-1] = scaled_pred\n        forecasts_lr[f\"{horizon}_days\"] = predictions\n    plot_forecast_1_day(df, df['Close'], forecasts_lr[\"1_days\"][0], \"LinearRegression\", stock_name, last_date)\n    plot_forecast_15_days(df, df['Close'], forecasts_lr[\"15_days\"], \"LinearRegression\", stock_name, last_date)\n    plot_forecast_30_days(df, df['Close'], forecasts_lr[\"30_days\"], \"LinearRegression\", stock_name, last_date)\n\n    # ARIMA\n    try:\n        p, d, q = estimate_arima_order(close_prices)\n        arima = ARIMA(close_prices, order=(p, d, q))\n        arima_model = arima.fit()\n        y_arima_pred = arima_model.predict(start=train_size+seq_len, end=len(close_prices)-1, type='levels')  # Updated 'typ' to 'type'\n        y_arima_pred = np.expm1(y_arima_pred)\n        model_scores[\"ARIMA\"] = evaluate_model(y_test_original, y_arima_pred, \"ARIMA Test\")\n        plot_predictions(df, df['Close'], df['Close'].values[seq_len:seq_len+train_size], y_arima_pred, \n                         f\"{stock_name} - ARIMA\", f\"charts/ARIMA/{stock_name}.png\", seq_len)\n\n        forecasts_arima = {}\n        for horizon in forecast_horizons:\n            forecast = arima_model.forecast(steps=horizon)\n            forecast = np.expm1(forecast)\n            forecast = [max(0, forecast[i] + (1 - np.exp(-0.1 * i)) * trend_slope + np.random.normal(0, volatility * forecast[i])) for i in range(len(forecast))]\n            forecasts_arima[f\"{horizon}_days\"] = forecast\n        plot_forecast_1_day(df, df['Close'], forecasts_arima[\"1_days\"][0], \"ARIMA\", stock_name, last_date)\n        plot_forecast_15_days(df, df['Close'], forecasts_arima[\"15_days\"], \"ARIMA\", stock_name, last_date)\n        plot_forecast_30_days(df, df['Close'], forecasts_arima[\"30_days\"], \"ARIMA\", stock_name, last_date)\n    except Exception as e:\n        print(f\"ARIMA failed for {stock_name}: {str(e)}\")\n        model_scores[\"ARIMA\"] = f\"ARIMA Failed: {str(e)}\"\n        forecasts_arima = {f\"{horizon}_days\": [] for horizon in forecast_horizons}\n\n    # LSTM\n    try:\n        X_train_lstm = X_train_scaled  # Shape: (samples, seq_len, n_features)\n        X_test_lstm = X_test_scaled\n\n        optimizers = {\n            'adam': Adam(learning_rate=0.001),\n            'rmsprop': RMSprop(learning_rate=0.001),\n            'sgd': SGD(learning_rate=0.001, momentum=0.9),\n            'adamax': Adamax(learning_rate=0.001)\n        }\n        \n        best_lstm_score = float('inf')\n        best_optimizer = None\n        best_model = None\n        best_history = None\n        optimizer_scores = {}\n        \n        for opt_name, optimizer in optimizers.items():\n            model_lstm = Sequential([\n                LSTM(128, return_sequences=True, input_shape=(seq_len, len(feature_columns))),\n                Dropout(0.3),\n                LSTM(64, return_sequences=True),\n                Dropout(0.3),\n                LSTM(32),\n                Dropout(0.3),\n                Dense(16),\n                Dense(1)\n            ])\n            model_lstm.compile(optimizer=optimizer, loss='mse')\n\n            es = EarlyStopping(patience=10, restore_best_weights=True)\n            history = model_lstm.fit(\n                X_train_lstm, \n                y_train_scaled, \n                epochs=100, \n                batch_size=32, \n                validation_split=0.2, \n                callbacks=[es], \n                verbose=1\n            )\n            \n            y_pred_lstm_scaled = model_lstm.predict(X_test_lstm)\n            y_pred_lstm = scaler.inverse_transform(\n                np.concatenate([np.zeros((len(y_pred_lstm_scaled), len(feature_columns)-1)), \n                                y_pred_lstm_scaled], axis=1))[:, 3]\n            y_pred_lstm = np.expm1(y_pred_lstm)\n            y_test_lstm = scaler.inverse_transform(\n                np.concatenate([np.zeros((len(y_test_scaled), len(feature_columns)-1)), \n                                y_test_scaled], axis=1))[:, 3]\n            y_test_lstm = np.expm1(y_test_lstm)\n            \n            scores = evaluate_model(y_test_lstm, y_pred_lstm, f\"LSTM ({opt_name}) Test\")\n            optimizer_scores[opt_name] = scores\n            \n            mse = mean_squared_error(y_test_lstm, y_pred_lstm)\n            if mse < best_lstm_score:\n                best_lstm_score = mse\n                best_optimizer = opt_name\n                best_model = model_lstm\n                best_history = history\n\n        plot_optimizer_comparison(optimizer_scores, stock_name)\n        model_scores[\"LSTM\"] = optimizer_scores[best_optimizer]\n        best_model.save(f\"trained_models/LSTM/{stock_name}.keras\")\n        plot_training_history(best_history, \"LSTM\", stock_name)\n\n        y_train_lstm_scaled = best_model.predict(X_train_lstm)\n        y_train_lstm = scaler.inverse_transform(\n            np.concatenate([np.zeros((len(y_train_lstm_scaled), len(feature_columns)-1)), \n                            y_train_lstm_scaled], axis=1))[:, 3]\n        y_train_lstm = np.expm1(y_train_lstm)\n        plot_predictions(df, df['Close'], y_train_lstm, y_pred_lstm, \n                         f\"{stock_name} - LSTM ({best_optimizer})\", f\"charts/LSTM/{stock_name}.png\", seq_len)\n\n        # LSTM Forecast\n        forecasts_lstm = {}\n        current_sequence = last_sequence_scaled.reshape((1, seq_len, len(feature_columns)))\n        for horizon in forecast_horizons:\n            predictions = []\n            temp_sequence = current_sequence.copy()\n            for i in range(horizon):\n                pred_scaled = best_model.predict(temp_sequence, verbose=0)\n                pred = scaler.inverse_transform(\n                    np.concatenate([np.zeros((1, len(feature_columns)-1)), \n                                    pred_scaled], axis=1))[:, 3]\n                pred = np.expm1(pred)[0]\n                pred = max(0, pred + (1 - np.exp(-0.1 * i)) * trend_slope + np.random.normal(0, volatility * pred))\n                predictions.append(pred)\n                scaled_pred = scaler.transform(\n                    np.concatenate([temp_sequence[0, -1, :-1].reshape(1, -1), \n                                    np.log1p([[pred]]).reshape(1, -1)], axis=1))\n                temp_sequence = np.roll(temp_sequence, -1, axis=1)\n                temp_sequence[0, -1, :] = scaled_pred[0]\n            forecasts_lstm[f\"{horizon}_days\"] = predictions\n        plot_forecast_1_day(df, df['Close'], forecasts_lstm[\"1_days\"][0], \"LSTM\", stock_name, last_date)\n        plot_forecast_15_days(df, df['Close'], forecasts_lstm[\"15_days\"], \"LSTM\", stock_name, last_date)\n        plot_forecast_30_days(df, df['Close'], forecasts_lstm[\"30_days\"], \"LSTM\", stock_name, last_date)\n    except Exception as e:\n        print(f\"LSTM failed for {stock_name}: {str(e)}\")\n        model_scores[\"LSTM\"] = f\"LSTM Failed: {str(e)}\"\n        forecasts_lstm = {f\"{horizon}_days\": [] for horizon in forecast_horizons}\n\n    return model_scores, {\"RandomForest\": forecasts_rf, \"XGBoost\": forecasts_xgb, \n                         \"LinearRegression\": forecasts_lr, \"ARIMA\": forecasts_arima, \"LSTM\": forecasts_lstm}\n\n# Process a single file\nfile_path = \"/kaggle/input/stock-1142025/processed_stock_data_scaled/HPG.json\"\nall_scores = {}\nall_forecasts = {}\n\nif os.path.exists(file_path):\n    print(f\"Processing {file_path}...\")\n    df = load_json_file(file_path)\n    print(f\"Loaded data with shape: {df.shape}\")\n    if df.shape[0] < 51:\n        print(f\"Skipping {file_path}: Insufficient data (<51 rows).\")\n    else:\n        result, forecasts = train_and_forecast(file_path, seq_len=50)\n        if result:\n            all_scores[os.path.basename(file_path)] = result\n            all_forecasts[os.path.basename(file_path)] = forecasts\n        else:\n            print(f\"No results returned for {file_path}.\")\nelse:\n    print(f\"File {file_path} does not exist.\")\n\n# Generate comparison plots\nfor metric in ['MAE', 'MSE', 'RMSE', 'R2']:\n    plot_model_comparison(all_scores, metric, f\"charts/comparison/{metric}_comparison.png\", stock_name=\"HPG\")\n\nimport pprint\npprint.pprint(all_scores)\npprint.pprint(all_forecasts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:17:33.851817Z","iopub.execute_input":"2025-04-21T13:17:33.852210Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/stock-1142025/processed_stock_data_scaled/HPG.json...\nLoaded data with shape: (2001, 17)\nX shape: (1951, 50, 16)\nY shape (1951, 1)\nRandom Forest Test MAE: 0.00\nRandom Forest Test MSE: 0.00\nRandom Forest Test RMSE: 0.00\nRandom Forest Test R²: 1.00\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"XGBoost Test MAE: 0.00\nXGBoost Test MSE: 0.00\nXGBoost Test RMSE: 0.00\nXGBoost Test R²: 1.00\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Linear Regression Test MAE: 0.00\nLinear Regression Test MSE: 0.00\nLinear Regression Test RMSE: 0.00\nLinear Regression Test R²: 1.00\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n","output_type":"stream"},{"name":"stdout","text":"ARIMA Test MAE: 0.47\nARIMA Test MSE: 0.22\nARIMA Test RMSE: 0.47\nARIMA Test R²: 0.00\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - loss: 0.0523 - val_loss: 0.0064\nEpoch 2/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0064 - val_loss: 0.0037\nEpoch 3/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0057 - val_loss: 0.0026\nEpoch 4/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0046 - val_loss: 0.0026\nEpoch 5/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0048 - val_loss: 0.0025\nEpoch 6/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0043 - val_loss: 0.0053\nEpoch 7/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0037 - val_loss: 0.0026\nEpoch 8/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0036 - val_loss: 0.0019\nEpoch 9/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0038 - val_loss: 0.0026\nEpoch 10/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0036 - val_loss: 0.0022\nEpoch 11/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0027 - val_loss: 0.0022\nEpoch 12/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0034 - val_loss: 0.0018\nEpoch 13/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0018\nEpoch 14/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0022 - val_loss: 0.0033\nEpoch 15/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0028\nEpoch 16/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0020\nEpoch 17/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0021\nEpoch 18/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0024 - val_loss: 0.0020\nEpoch 19/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0019\nEpoch 20/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0022 - val_loss: 0.0071\nEpoch 21/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0042 - val_loss: 0.0023\nEpoch 22/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - val_loss: 0.0016\nEpoch 23/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0024 - val_loss: 0.0015\nEpoch 24/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0022 - val_loss: 0.0016\nEpoch 25/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0018\nEpoch 26/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0017\nEpoch 27/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0022 - val_loss: 0.0021\nEpoch 28/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0015\nEpoch 29/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0017\nEpoch 30/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0029\nEpoch 31/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0025 - val_loss: 0.0015\nEpoch 32/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0032\nEpoch 33/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0026 - val_loss: 0.0017\nEpoch 34/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0014\nEpoch 35/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0020\nEpoch 36/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0038\nEpoch 37/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0027 - val_loss: 0.0015\nEpoch 38/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0014\nEpoch 39/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - val_loss: 0.0018\nEpoch 40/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0013\nEpoch 41/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0017 - val_loss: 0.0012\nEpoch 42/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0015\nEpoch 43/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0017 - val_loss: 0.0016\nEpoch 44/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - val_loss: 0.0041\nEpoch 45/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0021\nEpoch 46/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0023 - val_loss: 0.0018\nEpoch 47/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0023 - val_loss: 0.0012\nEpoch 48/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0013\nEpoch 49/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - val_loss: 0.0018\nEpoch 50/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0018 - val_loss: 0.0013\nEpoch 51/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - val_loss: 0.0013\nEpoch 52/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0028\nEpoch 53/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0020 - val_loss: 0.0013\nEpoch 54/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0019 - val_loss: 0.0016\nEpoch 55/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0019 - val_loss: 0.0024\nEpoch 56/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0020 - val_loss: 0.0018\nEpoch 57/100\n\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0021 - val_loss: 0.0015\n\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\nLSTM (adam) Test MAE: 0.00\nLSTM (adam) Test MSE: 0.00\nLSTM (adam) Test RMSE: 0.00\nLSTM (adam) Test R²: 1.00\nEpoch 1/100\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\n\ndata_folder = \"/kaggle/input/stock-1142025/processed_stock_data_scaled\"\noutput_dir = \"/kaggle/working/stock_charts/VCB\"\nos.makedirs(output_dir, exist_ok=True)\n\nfile_path = os.path.join(data_folder, \"VCB.json\")\nwith open(file_path, 'r') as f:\n    data = json.load(f)\ndf = pd.DataFrame(data)\ndf['Date'] = pd.to_datetime(df['Date'])\ndf = df.dropna()\n\ndef plot_price_with_moving_averages(df, title, output_path):\n    plt.figure(figsize=(14, 6))\n    plt.plot(df['Date'], df['Close'], label='Close Price', color='green')\n    plt.plot(df['Date'], df['SMA_5'], label='SMA 5', linestyle='--')\n    plt.plot(df['Date'], df['EMA_5'], label='EMA 5', linestyle=':')\n    plt.title(f\"{title} - Close Price with SMA & EMA\")\n    plt.xlabel('Date')\n    plt.ylabel('Normalized Price')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_rsi(df, title, output_path):\n    plt.figure(figsize=(14, 4))\n    plt.plot(df['Date'], df['RSI_14'], label='RSI 14', color='purple')\n    plt.axhline(0.7, linestyle='--', color='red', label='Overbought')\n    plt.axhline(0.3, linestyle='--', color='blue', label='Oversold')\n    plt.title(f\"{title} - RSI\")\n    plt.xlabel('Date')\n    plt.ylabel('RSI Value')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_macd(df, title, output_path):\n    plt.figure(figsize=(14, 4))\n    plt.plot(df['Date'], df['MACD'], label='MACD', color='black')\n    plt.plot(df['Date'], df['Signal_Line'], label='Signal Line', color='orange')\n    plt.axhline(0, linestyle='--', color='gray')\n    plt.title(f\"{title} - MACD\")\n    plt.xlabel('Date')\n    plt.ylabel('MACD Value')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_bollinger_bands(df, title, output_path):\n    plt.figure(figsize=(14, 6))\n    plt.plot(df['Date'], df['Close'], label='Close Price', color='green')\n    plt.plot(df['Date'], df['Upper_Band'], label='Upper Band', linestyle='--', color='red')\n    plt.plot(df['Date'], df['Lower_Band'], label='Lower Band', linestyle='--', color='blue')\n    plt.fill_between(df['Date'], df['Lower_Band'], df['Upper_Band'], alpha=0.1)\n    plt.title(f\"{title} - Bollinger Bands\")\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.legend()\n    plt.grid(True)\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_volume_obv(df, title, output_path):\n    fig, ax1 = plt.subplots(figsize=(14, 6))\n    ax1.set_xlabel('Date')\n    ax1.set_ylabel('Volume', color='skyblue')\n    ax1.bar(df['Date'], df['Volume'], label='Volume', alpha=0.5, color='skyblue')\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('OBV', color='orange')\n    ax2.plot(df['Date'], df['OBV'], label='OBV', color='orange')\n    plt.title(f\"{title} - Volume & OBV\")\n    fig.tight_layout()\n    plt.grid(True)\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_table(df, title, output_path):\n    fig, ax = plt.subplots(figsize=(14, 3))\n    ax.axis('off')\n    table = ax.table(cellText=df.head(5).values, colLabels=df.columns, loc='center', cellLoc='center')\n    table.scale(1.2, 1.5)\n    plt.title(f\"{title} - Sample Data Table\")\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\nplot_price_with_moving_averages(df, \"VCB\", os.path.join(output_dir, \"VCB_price_ma.png\"))\nplot_rsi(df, \"VCB\", os.path.join(output_dir, \"VCB_rsi.png\"))\nplot_macd(df, \"VCB\", os.path.join(output_dir, \"VCB_macd.png\"))\nplot_bollinger_bands(df, \"VCB\", os.path.join(output_dir, \"VCB_bollinger.png\"))\nplot_volume_obv(df, \"VCB\", os.path.join(output_dir, \"VCB_volume_obv.png\"))\nplot_table(df, \"VCB\", os.path.join(output_dir, \"VCB_table.png\"))\n\nprint(\"📊 Biểu đồ kỹ thuật cho mã cổ phiếu VCB:\")\nfor name in [\"VCB_price_ma\", \"VCB_rsi\", \"VCB_macd\", \"VCB_bollinger\", \"VCB_volume_obv\", \"VCB_table\"]:\n    display(Image(os.path.join(output_dir, f\"{name}.png\")))\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T13:17:20.780Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import Adam, RMSprop, SGD, Adamax\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom joblib import dump\nfrom tqdm import tqdm\n\nMODEL_NAMES = ['RandomForest', 'XGBoost', 'LinearRegression', 'ARIMA', 'LSTM']\nfor model in MODEL_NAMES:\n    os.makedirs(f\"trained_models/{model}\", exist_ok=True)\n    os.makedirs(f\"charts/{model}\", exist_ok=True)\nos.makedirs(\"charts/comparison\", exist_ok=True)\nos.makedirs(\"charts/LSTM/optimizers\", exist_ok=True)\n\ndef load_json_file(filepath):\n    with open(filepath, 'r') as f:\n        return pd.DataFrame(json.load(f))\n\ndef evaluate_model(y_true, y_pred, prefix=\"\"):\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    print(f\"{prefix} MAE: {mae:.2f}\")\n    print(f\"{prefix} MSE: {mse:.2f}\")\n    print(f\"{prefix} RMSE: {rmse:.2f}\")\n    print(f\"{prefix} R²: {r2:.2f}\")\n    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n\ndef plot_predictions(df, actual, train_pred, test_pred, title, output_path):\n    plt.figure(figsize=(15, 8))\n    plt.plot(df.index, actual, label=\"Actual Prices\", color='green')\n    plt.plot(df.index[:len(train_pred)], train_pred, label=\"Train Predictions\", color='orange')\n    plt.plot(df.index[len(train_pred):len(train_pred)+len(test_pred)], test_pred, label=\"Test Predictions\", color='teal')\n    plt.legend()\n    plt.title(title)\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Price\")\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef plot_training_history(history, model_name, stock_name):\n    plt.figure(figsize=(12, 6))\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.plot(history.history['val_loss'], label='Validation Loss')\n    plt.title(f'{stock_name} - {model_name} Training History')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid()\n    plt.savefig(f\"charts/{model_name}/{stock_name}_training_history.png\")\n    plt.close()\n\ndef plot_optimizer_comparison(optimizer_scores, stock_name):\n    plt.figure(figsize=(12, 6))\n    metrics = ['MAE', 'MSE', 'RMSE', 'R2']\n    for metric in metrics:\n        values = [scores[metric] for opt, scores in optimizer_scores.items()]\n        plt.plot(values, label=metric)\n    plt.xticks(range(len(optimizer_scores)), optimizer_scores.keys())\n    plt.title(f'{stock_name} - LSTM Optimizer Comparison')\n    plt.xlabel('Optimizer')\n    plt.ylabel('Metric Value')\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(f\"charts/LSTM/optimizers/{stock_name}_optimizer_comparison.png\")\n    plt.close()\n\ndef plot_model_comparison(all_scores, metric, output_path):\n    plt.figure(figsize=(12, 6))\n    for model_name in MODEL_NAMES:\n        values = [scores[model_name][metric] for filename, scores in all_scores.items()\n                 if isinstance(scores.get(model_name), dict)]\n        plt.plot(values, label=model_name)\n    plt.title(f'Model Comparison - {metric}')\n    plt.xlabel('Stock')\n    plt.ylabel(metric)\n    plt.legend()\n    plt.grid()\n    plt.tight_layout()\n    plt.savefig(output_path)\n    plt.close()\n\ndef train_models_on_file(filepath):\n    df = load_json_file(filepath)\n    df['Date'] = pd.to_datetime(df['Date'])\n    df.set_index('Date', inplace=True)\n    df = df.dropna()\n\n    if df.shape[0] < 100:\n        return\n\n    features = df.columns.difference(['Close'])\n    X = df[features].values\n    y = df['Close'].values\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)\n\n    model_scores = {}\n    stock_name = os.path.basename(filepath).replace('.json', '')\n\n    # Random Forest with regularization\n    rf = RandomForestRegressor(\n        n_estimators=100,\n        max_depth=10,\n        min_samples_split=5,\n        min_samples_leaf=2,\n        random_state=42\n    )\n    rf.fit(X_train, y_train)\n    dump(rf, f\"trained_models/RandomForest/{stock_name}.joblib\")\n    y_rf_train = rf.predict(X_train)\n    y_rf_test = rf.predict(X_test)\n    model_scores[\"RandomForest\"] = evaluate_model(y_test, y_rf_test, \"Random Forest Test\")\n    plot_predictions(df, y, y_rf_train, y_rf_test, f\"{stock_name} - Random Forest\", \n                    f\"charts/RandomForest/{stock_name}.png\")\n\n    # XGBoost with regularization\n    xgb = XGBRegressor(\n        n_estimators=100,\n        learning_rate=0.1,\n        max_depth=5,\n        min_child_weight=2,\n        reg_lambda=1.0,\n        random_state=42\n    )\n    xgb.fit(X_train, y_train)\n    dump(xgb, f\"trained_models/XGBoost/{stock_name}.joblib\")\n    y_xgb_train = xgb.predict(X_train)\n    y_xgb_test = xgb.predict(X_test)\n    model_scores[\"XGBoost\"] = evaluate_model(y_test, y_xgb_test, \"XGBoost Test\")\n    plot_predictions(df, y, y_xgb_train, y_xgb_test, f\"{stock_name} - XGBoost\", \n                    f\"charts/XGBoost/{stock_name}.png\")\n\n    # Linear Regression with feature scaling\n    scaler = MinMaxScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    lr = LinearRegression()\n    lr.fit(X_train_scaled, y_train)\n    dump(lr, f\"trained_models/LinearRegression/{stock_name}.joblib\")\n    y_lr_train = lr.predict(X_train_scaled)\n    y_lr_test = lr.predict(X_test_scaled)\n    model_scores[\"LinearRegression\"] = evaluate_model(y_test, y_lr_test, \"Linear Regression Test\")\n    plot_predictions(df, y, y_lr_train, y_lr_test, f\"{stock_name} - Linear Regression\", \n                    f\"charts/LinearRegression/{stock_name}.png\")\n\n    # ARIMA\n    try:\n        arima = ARIMA(y, order=(5,1,0))\n        arima_model = arima.fit()\n        y_arima_pred = arima_model.predict(start=len(y_train), end=len(y)-1, typ='levels')\n        model_scores[\"ARIMA\"] = evaluate_model(y_test, y_arima_pred, \"ARIMA Test\")\n        plot_predictions(df, y, y[:len(y_train)], y_arima_pred, f\"{stock_name} - ARIMA\", \n                        f\"charts/ARIMA/{stock_name}.png\")\n    except Exception as e:\n        model_scores[\"ARIMA\"] = f\"ARIMA Failed: {e}\"\n\n    # LSTM with different optimizers\n    try:\n        scaler = MinMaxScaler()\n        scaled_data = scaler.fit_transform(df[['Close']])\n        SEQ_LEN = 50\n        def create_sequences(data, seq_len):\n            X, y = [], []\n            for i in range(len(data) - seq_len):\n                X.append(data[i:i+seq_len])\n                y.append(data[i+seq_len])\n            return np.array(X), np.array(y)\n\n        X_lstm, y_lstm = create_sequences(scaled_data, SEQ_LEN)\n        X_train_lstm, X_test_lstm = X_lstm[:int(len(X_lstm)*0.8)], X_lstm[int(len(X_lstm)*0.8):]\n        y_train_lstm, y_test_lstm = y_lstm[:int(len(X_lstm)*0.8)], y_lstm[int(len(X_lstm)*0.8):]\n\n        # Try different optimizers\n        optimizers = {\n            'adam': Adam(learning_rate=0.001),\n            'rmsprop': RMSprop(learning_rate=0.001),\n            'sgd': SGD(learning_rate=0.001, momentum=0.9),\n            'adamax': Adamax(learning_rate=0.001)\n        }\n        \n        best_lstm_score = float('inf')\n        best_optimizer = None\n        best_model = None\n        best_history = None\n        optimizer_scores = {}\n        \n        for opt_name, optimizer in optimizers.items():\n            model_lstm = Sequential([\n                LSTM(64, return_sequences=True, input_shape=(X_train_lstm.shape[1], 1)),\n                Dropout(0.2),\n                LSTM(32),\n                Dropout(0.2),\n                Dense(16),\n                Dense(1)\n            ])\n            model_lstm.compile(optimizer=optimizer, loss='mse')\n\n            es = EarlyStopping(patience=5, restore_best_weights=True)\n            history = model_lstm.fit(\n                X_train_lstm, \n                y_train_lstm, \n                epochs=50, \n                batch_size=16, \n                validation_split=0.1, \n                callbacks=[es], \n                verbose=0\n            )\n            \n            y_pred_lstm = model_lstm.predict(X_test_lstm)\n            y_pred_lstm_inv = scaler.inverse_transform(y_pred_lstm)\n            y_test_lstm_inv = scaler.inverse_transform(y_test_lstm)\n            \n            scores = evaluate_model(y_test_lstm_inv, y_pred_lstm_inv, f\"LSTM ({opt_name}) Test\")\n            optimizer_scores[opt_name] = scores\n            \n            mse = mean_squared_error(y_test_lstm_inv, y_pred_lstm_inv)\n            if mse < best_lstm_score:\n                best_lstm_score = mse\n                best_optimizer = opt_name\n                best_model = model_lstm\n                best_history = history\n\n        # Plot optimizer comparison\n        plot_optimizer_comparison(optimizer_scores, stock_name)\n\n        # Save best model and plot results\n        model_scores[\"LSTM\"] = optimizer_scores[best_optimizer]\n        best_model.save(f\"trained_models/LSTM/{stock_name}.keras\")\n        plot_training_history(best_history, \"LSTM\", stock_name)\n\n        full_actual = scaler.inverse_transform(scaled_data[SEQ_LEN:])\n        full_pred = np.concatenate([\n            scaler.inverse_transform(best_model.predict(X_lstm[:len(X_train_lstm)])),\n            y_pred_lstm_inv\n        ])\n        plot_predictions(df.iloc[SEQ_LEN:], full_actual.flatten(), \n                        full_pred[:len(X_train_lstm)], full_pred[len(X_train_lstm):], \n                        f\"{stock_name} - LSTM ({best_optimizer})\", \n                        f\"charts/LSTM/{stock_name}.png\")\n\n    except Exception as e:\n        model_scores[\"LSTM\"] = f\"LSTM Failed: {e}\"\n\n    return model_scores\n\nfolder = \"/kaggle/input/stock-1142025/processed_stock_data_scaled\"\nall_scores = {}\nfor filename in tqdm(os.listdir(folder)):\n    if filename.endswith(\".json\"):\n        file_path = os.path.join(folder, filename)\n        result = train_models_on_file(file_path)\n        all_scores[filename] = result\n\n# Generate comparison plots\nfor metric in ['MAE', 'MSE', 'RMSE', 'R2']:\n    plot_model_comparison(all_scores, metric, f\"charts/comparison/{metric}_comparison.png\")\n\nimport pprint\npprint.pprint(all_scores)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-21T13:17:20.780Z"}},"outputs":[],"execution_count":null}]}